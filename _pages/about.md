---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


<span class='anchor' id='about-me'></span>

I am a Lecturer/Assistant Professor at the School of Mathematics, Physics and Computing, The University of Southern Queensland (UniSQ), Australia. Previously, I worked as an Associate Lecturer and Postdoctoral Research Fellow in School of EECS, The University of Queensland (UQ), working with Prof. Zi (Helen) Huang from August 2022 to January 2025. From January 2025 to April 2025, I worked as a research fellow in the ODeSI team of UQ Centre for Clinical Research (UQ-CCR). I obtained my PhD degree in 2023 from UQ, advised by Prof. Zi (Helen) Huang.

My research centers around developing generalizable and applicable ML/AI approaches. For fundemental research, I mainly explore the field of computer vision, particularly in **zero-shot learning**. For applied AI, I mainly conduct interdisciplinary research on **AI for Agriculture** and **AI for Healthcare**.

ğŸš€ I am now looking for highly motivated Ph.D. students, Master Students, Research Assistants, and Visiting Scholars working on computer vision and related areas. 

My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).

<span class='anchor' id='news'></span>
# ğŸ”¥ News

- *2025.04*: &nbsp; One paper on [Continual Learning](https://arxiv.org/pdf/2501.15454) has been accepted to IJCAI 2025!

- *2025.04*: &nbsp; One paper on [Continual Text-to-Video Retrieval](https://arxiv.org/pdf/2503.10111) has been accepted to SIGIR 2025!

- *2025.04*: &nbsp; I have joined the University of Southern Queensland as a Lecturer (Assistant Professor) after working as a Postdoc at UQ for two years!

- *2024.12*: &nbsp; One paper on [Source-free Open-Set Domain Adaptation](https://ojs.aaai.org/index.php/AAAI/article/view/34380/36535) has been accepted to AAAI 2025!

- *2024.10*: &nbsp; One paper on [Class-Agnostic Object Detection](https://arxiv.org/pdf/2406.14924) has been accepted to NeurIPS 2024!

- *2024.09*: &nbsp; Our [Coarse-to-Fine Prototype Refining Network for Point Cloud Completion and Reconstruction](https://arxiv.org/pdf/2409.08443) for CVPPA@ECCV 2025 challenge won the first place solution! 

- *2024.07*: &nbsp; One paper on [In-the-wild Multimodal Disease Recognition](https://dl.acm.org/doi/10.1145/3664647.3680599) has been accepted to ACM MM 2024!
- 
# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### 2025

- ![SVIP image](https://uqzhichen.github.io/images/image.png)  
  **[SVIPâ€¯â€“â€¯Semantically Contextualized Visual Patches for Zeroâ€‘Shot Learning](https://arxiv.org/pdf/2503.10252)**  
  **Zhiâ€¯Chen**,â€¯Zechengâ€¯Zhao,â€¯Jingcaiâ€¯Guo,â€¯Jingjingâ€¯Li,â€¯Ziâ€¯Huang  
  *preâ€‘print* â€” **ZSL**  
  [arXiv](https://arxiv.org/abs/2503.10252)

- ![Retrieval image](https://uqzhichen.github.io/images/stablefusion.png)  
  **[Continual Textâ€‘toâ€‘Video Retrieval with Frame Fusion and Taskâ€‘Aware Routing](https://arxiv.org/pdf/2503.10111)**  
  Zechengâ€¯Zhao,â€¯**Zhiâ€¯Chen**,â€¯Ziâ€¯Huang,â€¯Shaziaâ€¯Sadiq,â€¯Tongâ€¯Chen  
  *SIGIRâ€¯2025* (COREâ€¯A*) â€” **CL**  
  [arXiv](https://arxiv.org/pdf/2503.10111)

- ![CIL image](https://uqzhichen.github.io/images/continual1.png)  
  **[On the Discrimination and Consistency for Exemplarâ€‘Free Class Incremental Learning](https://arxiv.org/pdf/2501.15454)**  
  Tianqiâ€¯Wang,â€¯Jingcaiâ€¯Guo,â€¯Depengâ€¯Li,â€¯**Zhiâ€¯Chen**  
  *preâ€‘print* â€” **CL**  
  [arXiv](https://arxiv.org/pdf/2501.15454)

- ![DA image](https://uqzhichen.github.io/images/aaai25.png)  
  **[Dynamic Target Distribution Estimation for Sourceâ€‘free Openâ€‘Set Domain Adaptation](https://ojs.aaai.org/index.php/AAAI/article/view/34380/36535)**  
  Zhiqiâ€¯Yu,â€¯Zhichaoâ€¯Liao,â€¯Jingjingâ€¯Li,â€¯**Zhiâ€¯Chen**,â€¯Leiâ€¯Zhu  
  *AAAIâ€¯2025* (COREâ€¯A*) â€” **DA**  
  [paper](https://ojs.aaai.org/index.php/AAAI/article/view/34380/36535)

- ![PIâ€‘ML image](https://uqzhichen.github.io/images/placeholder.png)  
  **Physicsâ€‘Informed Machine Learning Offers Multiâ€‘Scale Remote Estimation of Wheat Biomass Dynamics**  
  Qiaominâ€¯Chen*,â€¯**Zhiâ€¯Chen***â€¯etâ€¯al.  
  *IEEEâ€¯IGARSSâ€¯2025* â€” **AIÂ forÂ Science**  
  (paper link TBA)


### 2024

- ![DiPEx image](https://uqzhichen.github.io/images/dipex.png)  
  **[DiPEx: Dispersing Prompt Expansion for Classâ€‘Agnostic Object Detection](https://arxiv.org/pdf/2406.14924)**  
  Jiaâ€¯Syuenâ€¯Lim,â€¯Zhuoxiaoâ€¯Chen,â€¯**Zhiâ€¯Chen**,â€¯Mahsaâ€¯Baktashmotlagh,â€¯Xinâ€¯Yu,â€¯Ziâ€¯Huang,â€¯Yadanâ€¯Luo  
  *NeurIPSâ€¯2024*  
  [arXiv](https://arxiv.org/pdf/2406.14924)Â /Â [code](https://github.com/jason-lim26/DiPEx/tree/master)

- ![MVPDR image](https://uqzhichen.github.io/images/mvpdr.png)  
  **[Benchmarking Inâ€‘theâ€‘wild Multimodal Disease Recognition and a Versatile Baseline](https://dl.acm.org/doi/10.1145/3664647.3680599)**  
  Tianqiâ€¯Wei,â€¯**Zhiâ€¯Chen**,â€¯Ziâ€¯Huang,â€¯Xinâ€¯Yu  
  *ACMÂ MMâ€¯2024* (COREâ€¯A*) â€” **AIÂ forÂ Science**  
  [arXiv](https://arxiv.org/abs/2408.03120)Â /Â [code](https://github.com/tqwei05/MVPDR)Â /Â [dataset](https://huggingface.co/datasets/uqtwei2/PlantWild)

- ![CFâ€‘PRNet image](https://uqzhichen.github.io/images/cfprnet.png)  
  **[CFâ€‘PRNet: Coarseâ€‘toâ€‘Fine Prototype Refining Network for Point Cloud Completion and Reconstruction](https://arxiv.org/pdf/2409.08443)**  
  **Zhiâ€¯Chen**,â€¯Tianqiâ€¯Wei,â€¯Zechengâ€¯Zhao,â€¯Jiaâ€¯Syuenâ€¯Lim,â€¯Yadanâ€¯Luo,â€¯Huâ€¯Zhang,â€¯Xinâ€¯Yu,â€¯Scottâ€¯Chapman,â€¯Ziâ€¯Huang  
  *1stâ€‘place solution, CVPPA@ECCVâ€¯2024* â€” **AIÂ forÂ Science**  
  [arXiv](https://arxiv.org/pdf/2409.08443)Â /Â [code](https://github.com/uqzhichen/CF-PRNet)Â /Â [competition](https://cvppa2024.github.io/challenges/)

- ![TAP image](https://uqzhichen.github.io/images/tap.png)  
  **[TrackÂ AnyÂ Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs](https://arxiv.org/pdf/2411.06702)**  
  Jiaâ€¯Syuenâ€¯Lim,â€¯Yadanâ€¯Luo,â€¯**Zhiâ€¯Chen**,â€¯Tianqiâ€¯Wei,â€¯Scottâ€¯Chapman,â€¯Ziâ€¯Huang  
  *2ndâ€‘place solution, CVPPA@ECCVâ€¯2024* â€” **AIÂ forÂ Science**  
  [paper](https://arxiv.org/pdf/2411.06702)Â /Â [competition](https://cvppa2024.github.io/challenges/)

- ![PlantSeg image](https://uqzhichen.github.io/images/plantseg.png)  
  **[PlantSeg: A Largeâ€‘Scale Inâ€‘theâ€‘wild Dataset for Plant Disease Segmentation](https://arxiv.org/pdf/2409.04038)**  
  Tianqiâ€¯Wei,â€¯**Zhiâ€¯Chen**,â€¯Xinâ€¯Yu,â€¯Scottâ€¯Chapman,â€¯Paulâ€¯Melloy,â€¯Ziâ€¯Huang  
  *preâ€‘print* â€” **AIÂ forÂ Science**  
  [arXiv](https://arxiv.org/pdf/2409.04038)Â /Â [code](https://github.com/tqwei05/PlantSeg)Â /Â [dataset](https://zenodo.org/records/13762907)

- ![SnapÂ &Â Diagnose image](https://uqzhichen.github.io/images/snap.png)  
  **[Snapâ€¯andâ€¯Diagnose: An Advanced Multimodal Retrieval System for Identifying Plant Diseases in the Wild](https://arxiv.org/pdf/2408.14723)**  
  Tianqiâ€¯Wei,â€¯**Zhiâ€¯Chen**,â€¯Xinâ€¯Yu,â€¯Scottâ€¯Chapman,â€¯Paulâ€¯Melloy,â€¯Ziâ€¯Huang  
  *ACMÂ Multimedia in Asiaâ€¯2024* â€” **AIÂ forÂ Science**  
  [paper](https://arxiv.org/pdf/2408.14723)

- ![FastEdit image](https://uqzhichen.github.io/images/fastedit.png)  
  **[FastEdit: Fast Textâ€‘Guided Singleâ€‘Image Editing via Semanticâ€‘Aware Diffusion Fineâ€‘Tuning](https://arxiv.org/pdf/2408.03355)**  
  **Zhiâ€¯Chen**,â€¯Zechengâ€¯Zhao,â€¯Yadanâ€¯Luo,â€¯Ziâ€¯Huang  
  *preâ€‘print* â€” **ImageÂ Editing**  
  [arXiv](https://arxiv.org/pdf/2408.03355)Â /Â [code](https://github.com/JasonCodeMaker/FastEdit)

- ![PAKDDÂ RL image](https://uqzhichen.github.io/images/pakdd.png)  
  **[Towards Costâ€‘Efficient Federated Multiâ€‘Agent Reinforcement Learning with Learnable Aggregation](https://openreview.net/pdf?id=nbN8Ilbg8c)**  
  Yiâ€¯Zhang,â€¯Senâ€¯Wang,â€¯**Zhiâ€¯Chen**,â€¯Xuweiâ€¯Xu,â€¯Stanoâ€¯Funiak,â€¯Frankâ€¯deâ€¯Hoog,â€¯Jiajunâ€¯Liu  
  *PAKDDâ€¯2024* â€” **RL** (Best Student Paper)  
  [paper](https://openreview.net/pdf?id=nbN8Ilbg8c)

- ![ZSLÂ survey image](https://uqzhichen.github.io/images/survey2.png)  
  **[On the Elementâ€‘Wise Representation and Reasoning in Zeroâ€‘Shot Image Recognition: A Systematic Survey](https://arxiv.org/pdf/2408.04879)**  
  Jingcaiâ€¯Guo,â€¯Zhijieâ€¯Rao,â€¯**Zhiâ€¯Chen**,â€¯Songâ€¯Guo,â€¯Jingrenâ€¯Zhou,â€¯Dachengâ€¯Tao  
  *preâ€‘print* â€” **ZSL**  
  [arXiv](https://arxiv.org/pdf/2408.04879)

- ![Diabetes image](https://uqzhichen.github.io/images/diabetes2024.png)  
  **[Secondary Analysis of Newly Diagnosed Typeâ€¯2 Diabetes Subgroups and Treatment Responses in theÂ MARCH Cohort](https://www.sciencedirect.com/science/article/pii/S1871402123002321)**  
  Weihaoâ€¯Wang,â€¯Xiaobeiâ€¯Li,â€¯Feiâ€¯Chen,â€¯Ranâ€¯Wei,â€¯**Zhiâ€¯Chen**,â€¯Jingjingâ€¯Li,â€¯Jingtaoâ€¯Qiao,â€¯Qiâ€¯Pan,â€¯Wenjingâ€¯Yang,â€¯Lixinâ€¯Guo  
  *DiabetesÂ &â€¯MetabolicÂ Syndrome: Clinical Research & ReviewsÂ 2024* â€” **AIÂ forÂ Science**  
  [paper](https://www.sciencedirect.com/science/article/pii/S1871402123002321)


### 2023




# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
